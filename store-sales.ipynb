{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:56:19.193872Z","iopub.execute_input":"2025-07-25T07:56:19.194198Z","iopub.status.idle":"2025-07-25T07:56:19.201518Z","shell.execute_reply.started":"2025-07-25T07:56:19.194174Z","shell.execute_reply":"2025-07-25T07:56:19.200536Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/store-sales-time-series-forecasting/oil.csv\n/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv\n/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\n/kaggle/input/store-sales-time-series-forecasting/stores.csv\n/kaggle/input/store-sales-time-series-forecasting/train.csv\n/kaggle/input/store-sales-time-series-forecasting/test.csv\n/kaggle/input/store-sales-time-series-forecasting/transactions.csv\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nimport tensorflow as tf\nimport keras_tuner as kt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:08:12.303526Z","iopub.execute_input":"2025-07-25T09:08:12.303904Z","iopub.status.idle":"2025-07-25T09:08:12.309559Z","shell.execute_reply.started":"2025-07-25T09:08:12.303841Z","shell.execute_reply":"2025-07-25T09:08:12.308658Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"\n\n## Detect hardware (CPU/GPU/TPU), setup environment and return appropriate distribution strategy\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='local') # set tpu is local as it should be available in the VM\n    print('✅ Running on TPU ', tpu.master())\nexcept:\n    print('❌ Using CPU/GPU')\n    tpu = None\n\nif tpu:\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T08:46:50.101360Z","iopub.execute_input":"2025-07-25T08:46:50.102429Z","iopub.status.idle":"2025-07-25T08:46:50.538340Z","shell.execute_reply.started":"2025-07-25T08:46:50.102393Z","shell.execute_reply":"2025-07-25T08:46:50.537410Z"}},"outputs":[{"name":"stdout","text":"❌ Using CPU/GPU\nREPLICAS:  1\n","output_type":"stream"},{"name":"stderr","text":"2025-07-25 08:46:50.483957: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/train.csv')\ntest = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/test.csv')\nstores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\ntransactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')\nholidays = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')\noil_price = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:56:19.226136Z","iopub.execute_input":"2025-07-25T07:56:19.226536Z","iopub.status.idle":"2025-07-25T07:56:21.565685Z","shell.execute_reply.started":"2025-07-25T07:56:19.226508Z","shell.execute_reply":"2025-07-25T07:56:21.564456Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"#preprocessing holidays data\nholidays.head()\n#filtering out transferred holidays\nholidays = holidays[holidays['transferred'] == False]\nholidays = holidays.replace(to_replace=['Transfer', 'Bridge', 'Additional'], value='Holiday')\nholidays['Holiday_Gen'] = holidays['type']=='Holiday' #holiday or not\nholidays['Event_Gen'] = holidays['type']=='Event' #event or not\nholidays['Work_Day_Gen'] = holidays['type']=='Work Day'\nholidays.drop(columns=[\"type\", \"transferred\"], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:56:21.567199Z","iopub.execute_input":"2025-07-25T07:56:21.567789Z","iopub.status.idle":"2025-07-25T07:56:21.581137Z","shell.execute_reply.started":"2025-07-25T07:56:21.567751Z","shell.execute_reply":"2025-07-25T07:56:21.580060Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"#preprocessing oil price data\noil_price.loc[0, \"dcoilwtico\"] = oil_price.loc[1, \"dcoilwtico\"] #fill in the only nan\noil_price.ffill(inplace=True)\noil_price.bfill(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:56:21.582266Z","iopub.execute_input":"2025-07-25T07:56:21.582641Z","iopub.status.idle":"2025-07-25T07:56:21.601210Z","shell.execute_reply.started":"2025-07-25T07:56:21.582540Z","shell.execute_reply":"2025-07-25T07:56:21.600327Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"#data preprocessing\n\nscalers = {'scaler_onpromotion' : MinMaxScaler(), 'scaler_day' : MinMaxScaler(), 'scaler_month' : MinMaxScaler(),\n           'scaler_year' : MinMaxScaler(), 'scaler_dcoilwtico' : MinMaxScaler(), 'scaler_DoW' : MinMaxScaler()}\n\ndef preprocess_data(data, stores, transactions, holidays, oil_price, fit_scaler, scalers):\n    #data will be empty before adding all dfs\n    #merge stores into data to substitute store ids with location data\n    data = data.merge(stores, how = 'left', left_on = 'store_nbr', right_on = 'store_nbr')\n\n    data = data.merge(holidays, how = 'left', left_on = 'date', right_on = 'date')\n    data['Holiday'] = ((data['Holiday_Gen'] == True)\n                      & ((data['city'] == data['locale_name'])\n                        | (data['state'] == data['locale_name'])\n                        | ('Ecuador' == data['locale_name']))).astype(dtype='float64')\n    data['Event'] = ((data['Event_Gen'] == True)\n                       & ((data['city'] == data['locale_name'])\n                          | (data['state'] == data['locale_name'])\n                          | ('Ecuador' == data['locale_name']))).astype(dtype='float64')\n    data['Work_Day'] = ((data['Work_Day_Gen'] == True)\n                        & ((data['city'] == data['locale_name'])\n                           | (data['state'] == data['locale_name'])\n                           | ('Ecuador' == data['locale_name']))).astype(dtype='float64')\n\n    #add oil prices to data + fill missing values\n    data = data.merge(oil_price, how = 'left', left_on = 'date', right_on = 'date')\n    data.sort_values('id', inplace = True) #sort rows by id, modify original df with no copy made\n    data['dcoilwtico'] = data['dcoilwtico'].ffill() #forward fill\n\n    #add transactions to data\n    data = data.merge(transactions, how = 'left', left_on = ['date', 'store_nbr'], right_on = ['date', 'store_nbr']) #join data and transactions  where date and store_nbr match\n\n    #modify date, pay date info\n    data['date'] = pd.to_datetime(data['date']) #turn into math friendly datetime data\n    data['day'] = data['date'].dt.day #extract the day from the date and put in a column\n    data['month'] = data['date'].dt.month #extract month and put in a column\n    data['year'] = data['date'].dt.year #extract year and put in a column\n    data['day_of_week'] = data['date'].dt.day_name() #get day of the week NAME (strings)\n    data['DoW'] = data['date'].dt.dayofweek #get day of the week as number (monday = 0, int)\n    #pay days are on the first and 16th\n    data['Pay_Day'] = ((data['date'].dt.day == 1) | (data['date'].dt.day == 16)).astype(dtype = 'float64') #check if the day is 1 or 16 (if its payday)\n    #there was an earthquake on 2016-04-16\n    data['Earthquake'] = (data['date'] == '2016-04-16').astype(dtype = 'float64')\n\n    #fill nans with proper values\n    data.fillna({\n        'Holiday_Gen': False, #fill all holiday gen nans with false etc\n        'Event_Gen': False,\n        'Work_Day_Gen': False,\n        'Holiday': 0.0,\n        'Event': 0.0,\n        'Work_Day': 0.0,\n        'Pay_Day': 0.0,\n        'Earthquake': 0.0,\n        'transactions': 0.0\n    }, inplace=True) #change original df directly, no copy is made\n\n    #onehotencoding categorical features: \n    encoder = OneHotEncoder()\n    One_Hot_Encoding = pd.DataFrame(encoder.fit_transform(data[['store_nbr', 'family', 'day_of_week']]).toarray(),\n                                   columns = encoder.get_feature_names_out())\n    #creates a dataframe of the array output(toarray) of the encoder results\n\n    #scaling numerical values\n\n    #if a specific FIT scaler is given, fitting it then transforming it outside the if\n    if fit_scaler: \n        scalers['scaler_dcoilwtico'].fit(oil_price[['dcoilwtico']]) #MinMaxScaler fit on dcoilwtico from oil_price\n        for col in ['onpromotion', 'day', 'month', 'year', 'DoW']:\n            scalers[f'scaler_{col}'].fit(data[[col]])\n    data[['dcoilwtico']] = scalers['scaler_dcoilwtico'].transform(data[['dcoilwtico']])\n    for col in ['onpromotion', 'day', 'month', 'year', 'DoW']:\n        data[[col]] = scalers[f'scaler_{col}'].transform(data[[col]])\n\n    #feature vectors and labels vector\n    features = data[['onpromotion', 'dcoilwtico', 'Holiday', 'Event', 'Work_Day', 'Earthquake', 'day', 'month', 'year']]\n    features = features.merge(One_Hot_Encoding, how = 'left', left_index = True, right_index = True) #add encoded features to features\n    #left_index = the left joining key will be the index\n    #right_index = the right joining key will be the index\n\n    if 'sales' in data.columns:\n        labels = data[['sales']]\n    else: labels = []\n\n\n    return features, labels, scalers, data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:56:21.603346Z","iopub.execute_input":"2025-07-25T07:56:21.603747Z","iopub.status.idle":"2025-07-25T07:56:21.626862Z","shell.execute_reply.started":"2025-07-25T07:56:21.603711Z","shell.execute_reply":"2025-07-25T07:56:21.626096Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"\ntrainvaltes_feat, trainvaltes_label, scalers, datat = preprocess_data(train, stores, transactions, holidays, oil_price, True, scalers)\n#true for fit scalers\n\ntest_feat, test_label, _, data = preprocess_data(test, stores, transactions, holidays, oil_price, False, scalers)\n#false for fit scalers because they have already been fitted for the train data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:56:21.627965Z","iopub.execute_input":"2025-07-25T07:56:21.628638Z","iopub.status.idle":"2025-07-25T07:56:46.187293Z","shell.execute_reply.started":"2025-07-25T07:56:21.628610Z","shell.execute_reply":"2025-07-25T07:56:46.185930Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1817755179.py:46: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  data.fillna({\n/tmp/ipykernel_36/1817755179.py:46: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  data.fillna({\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# split into train val tes\ntrain_feat, val_feat, train_label, val_label = train_test_split(trainvaltes_feat, trainvaltes_label, test_size = 0.005, random_state = 33)\n#test_size = validation size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:56:46.188210Z","iopub.execute_input":"2025-07-25T07:56:46.188520Z","iopub.status.idle":"2025-07-25T07:56:51.092912Z","shell.execute_reply.started":"2025-07-25T07:56:46.188500Z","shell.execute_reply":"2025-07-25T07:56:51.091985Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"#models\nsvr = SVR()\nrfr = RandomForestRegressor(n_estimators = 100)\nkneigh = KNeighborsRegressor(n_neighbors = 5)\ngbr = GradientBoostingRegressor(n_estimators = 200)\nxbg = xgb.XGBRegressor()\n\n#model space\nEstimatorStr = {1 : 'svr', 2 : 'rfr', 3 : 'kneigh', 4 : 'gbr', 5 : 'xgb', 6 : 'tbd'}\nEstimatorMdl = {1 : svr, 2 : rfr, 3 : kneigh, 4 : gbr, 5 : xgb}\n\n#hyperparameter space\n#grid spaces = include multiple combinations of values\n#single spaces = staatic combinations\n\nparam_grid_svr = {'svr__C' : np.linspace(1, 10, 5), #C: Regularization strength. higher = less regularization\n                  'svr_degree' : np.arange(2, 6, 1), #degree is only used for poly kernel. including for flexibility\n                  'svr_kernel' : ['rbf']} #kernel (radial basis function)\nparam_single_svr = {'svr__C': [1],\n                   'svr_degree' : [3],\n                   'svr_kernel' : ['rbf']}\n\nparam_grid_rfr = {'randomforestregressor__n_estimators' : np.arange(50, 250, 50)}\nparam_single_rfr = {'randomforestregressor__n_estimators' : [100]}\n\nparam_grid_neigh = {'kneighborsregressor__n_neihbors' : np.arange(2, 12, 2)}\nparam_single_neigh = {'kneighborsregressor__n_neihbors' : [5]}\n\nparam_grid_gbr = {'gradienboostingregressor__n_estimators' : np.arange(50, 250, 50),\n                 'gradientboostingregressor__max_depth' : np.arange(2, 6, 1)}\nparam_single_gbr = {'gradientboostingregressor__n_estimators' : [100],\n                   'gradientboostingregressor__max_depth' : [3]}\n\nparam_grid_xgb = {'xgbregressor__n_estimators' : np.arange(50, 250, 50),\n                 'xgbregressor__max_depth' : np.arange(2, 6, 1)}\nparam_single_xgb = {'xgbregressor__n_estimators' : [100], 'xgbregressor__max_depth' : [3]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T08:22:17.613846Z","iopub.execute_input":"2025-07-25T08:22:17.615208Z","iopub.status.idle":"2025-07-25T08:22:17.633458Z","shell.execute_reply.started":"2025-07-25T08:22:17.615172Z","shell.execute_reply":"2025-07-25T08:22:17.632219Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"#Neural network model (deep neural network)\ndef build_network(hp):\n    #define the input layer and what the model expects\n    inp = tf.keras.Input(shape = (len(train_feat.columns),),name = 'input')\n\n    #apply a FC (fully connected) layer then a regression\n\n    reg = None #no regularizer\n    #regularizers help reduce overfitting, memorizing etc\n    do = hp.Float(name = 'dropout', min_value = 0.04, max_value = 0.04, default = 0.04) #dropout rate 0.04 = 4% train data thrown\n    x = inp\n    layers = hp.Int(name = 'layers', min_value = 5, max_value = 5, default = 5) #how many hidden layers\n    units = hp.Int(name = 'units', min_value = 768, max_value = 768, default = 768) #how many neurons per layer\n    act = 'relu' #Rectified Linear Unit, it keeps positive values, kills the negatives.\n\n    for i, units in enumerate([units for i in range(layers)]):\n        x = tf.keras.layers.Dense(units, activation = act, kernel_regularizer = reg, name=f'fc{i+1}')(x)\n        x = tf.keras.layers.Dropout(do, name = f'do{i+1}')(x)\n    out = tf.keras.layers.Dense(1, activation = act, kernel_regularizer = reg, name = 'regression')(x) #final regression output\n    model = tf.keras.Model(inputs = inp, outputs = out, name = 'DNN_Network') #wrap it up in a keras object model\n\n    lr_tune = hp.Float(name = 'learning_rate', min_value = 5e-5, max_value = 5e-3, sampling = 'log', default = 5e-4) #1e-3\n    optimizer = tf.keras.optimizers.Adam(lr_tune) #adam optimizer with tunable learning rate\n    model.compile(optimizer = optimizer, loss = tf.keras.losses.msle, metrics = ['msle']) #mean squared logarithmic error\n\n    return model\n\nwith strategy.scope():\n    model_nn = build_network(kt.HyperParameters())\n#distributed training under strategy \n\nmodel_nn.summary() #show layer by layer summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T08:49:27.085457Z","iopub.execute_input":"2025-07-25T08:49:27.085795Z","iopub.status.idle":"2025-07-25T08:49:27.204492Z","shell.execute_reply.started":"2025-07-25T08:49:27.085772Z","shell.execute_reply":"2025-07-25T08:49:27.203670Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"DNN_Network\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"DNN_Network\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m103\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │        \u001b[38;5;34m79,872\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ do1 (\u001b[38;5;33mDropout\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │       \u001b[38;5;34m590,592\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ do2 (\u001b[38;5;33mDropout\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc3 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │       \u001b[38;5;34m590,592\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ do3 (\u001b[38;5;33mDropout\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc4 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │       \u001b[38;5;34m590,592\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ do4 (\u001b[38;5;33mDropout\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc5 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │       \u001b[38;5;34m590,592\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ do5 (\u001b[38;5;33mDropout\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ regression (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m769\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">103</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">79,872</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ do1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ do2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ do3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ do4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ fc5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ do5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ regression (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,443,009\u001b[0m (9.32 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,443,009</span> (9.32 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,443,009\u001b[0m (9.32 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,443,009</span> (9.32 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":56},{"cell_type":"code","source":"## Training configurations\n\nTUNING = True\nTRAINING = False\nML = False\nNN = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:07:38.927795Z","iopub.execute_input":"2025-07-25T09:07:38.928122Z","iopub.status.idle":"2025-07-25T09:07:38.933271Z","shell.execute_reply.started":"2025-07-25T09:07:38.928098Z","shell.execute_reply":"2025-07-25T09:07:38.932196Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"## Hyperparameter search or single fitting for machine learning methods\n\nfor i_TypEstimator in [1, 2, 3, 4, 5]: # Choosing models to tune {1: svc, 2: rfc, 3: kneigh, 4: gbc, 5: xbg} 1, 2, 3, 4, 5\n    pipeline = make_pipeline(EstimatorMdl[i_TypEstimator])\n    if TUNING and ML:\n        param_grid = globals()[f'param_grid_{EstimatorStr[i_TypEstimator]}']\n        grid = GridSearchCV(pipeline, param_grid, scoring='neg_mean_squared_log_error', verbose=3, cv=skf5)\n        grid.fit(trainvaltes_feat, np.array(trainvaltes_label).ravel())\n        param_single = grid.best_params_\n        for item in param_single:\n            param_single[item] = [grid.best_params_[item]]\n    if not TUNING and ML:\n        param_single = globals()[f'param_single_{EstimatorStr[i_TypEstimator]}']\n\n    if ML:\n        grid = GridSearchCV(pipeline, param_single, scoring='neg_mean_squared_log_error', verbose=3)\n        grid.fit(train_feat, np.array(train_label).ravel())\n        print(grid.best_params_)\n        model_ml = grid.best_estimator_\n        print(f'Train score: {model_ml.score(train_feat, np.array(train_label).ravel())}')\n        print(f'Validation score: {model_ml.score(val_feat, np.array(val_label).ravel())}')\n        globals()[f'model_{EstimatorStr[i_TypEstimator]}'] = model_ml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:08:14.751202Z","iopub.execute_input":"2025-07-25T09:08:14.751570Z","iopub.status.idle":"2025-07-25T09:08:14.760505Z","shell.execute_reply.started":"2025-07-25T09:08:14.751545Z","shell.execute_reply":"2025-07-25T09:08:14.759310Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"## Training parameters for DNN network\n\nepochs = 100\n\n# Callback functions\nlr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5, verbose=1, monitor='val_msle', restore_best_weights=True)\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1, monitor='val_msle', restore_best_weights=True)\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-5 * 10**(epoch / 20)) # Find starting learning\n\ntuner = kt.RandomSearch(\n    hypermodel=build_network,\n    objective='val_msle',\n    max_trials=10,\n    executions_per_trial=1,\n    overwrite=True,\n    directory=\"tuner\",\n    project_name=\"StoreSales\",\n    distribution_strategy = strategy,\n)\n\n# tuner = kt.Hyperband(\n#     hypermodel=build_network,\n#     objective='val_msle',\n#     max_epochs=50,\n#     factor=3,\n#     hyperband_iterations=1,\n#     directory=\"tuner\",\n#     project_name=\"StoreSales\",\n#     distribution_strategy = strategy,\n# )\n\ntuner.search_space_summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:08:33.975012Z","iopub.execute_input":"2025-07-25T09:08:33.975379Z","iopub.status.idle":"2025-07-25T09:08:34.080098Z","shell.execute_reply.started":"2025-07-25T09:08:33.975355Z","shell.execute_reply":"2025-07-25T09:08:34.079005Z"}},"outputs":[{"name":"stdout","text":"Search space summary\nDefault search space size: 4\ndropout (Float)\n{'default': 0.04, 'conditions': [], 'min_value': 0.04, 'max_value': 0.04, 'step': None, 'sampling': 'linear'}\nlayers (Int)\n{'default': 5, 'conditions': [], 'min_value': 5, 'max_value': 5, 'step': 1, 'sampling': 'linear'}\nunits (Int)\n{'default': 768, 'conditions': [], 'min_value': 768, 'max_value': 768, 'step': 1, 'sampling': 'linear'}\nlearning_rate (Float)\n{'default': 0.0005, 'conditions': [], 'min_value': 5e-05, 'max_value': 0.005, 'step': None, 'sampling': 'log'}\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"## Train model by Tuner or Fit\n\nif TUNING and NN:\n    tuner.search(train_feat, train_label, validation_data=(val_feat, val_label), batch_size=1024,\n                 epochs=epochs, callbacks=[lr_scheduler, early_stopping_cb])\n    best_models = tuner.get_best_models(num_models=2)\n    model_nn = best_models[0]\n    model_nn.summary()\n    tuner.results_summary()\n\nif TRAINING and NN:\n    history = model_nn.fit(train_feat, train_label, validation_data=(val_feat, val_label), batch_size=1024,\n                        epochs=epochs, callbacks=[lr_scheduler, early_stopping_cb])\n    model_nn.evaluate(val_feat, val_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T09:08:43.074593Z","iopub.execute_input":"2025-07-25T09:08:43.074972Z"}},"outputs":[{"name":"stdout","text":"\nSearch: Running Trial #1\n\nValue             |Best Value So Far |Hyperparameter\n0.04              |0.04              |dropout\n5                 |5                 |layers\n768               |768               |units\n0.00014788        |0.00014788        |learning_rate\n\nEpoch 1/100\n\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 149ms/step - loss: 1.9085 - msle: 1.9085 - val_loss: 0.4164 - val_msle: 0.4164 - learning_rate: 1.4788e-04\nEpoch 2/100\n\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 147ms/step - loss: 0.3696 - msle: 0.3696 - val_loss: 0.3017 - val_msle: 0.3017 - learning_rate: 1.4788e-04\nEpoch 3/100\n\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 148ms/step - loss: 0.2741 - msle: 0.2741 - val_loss: 0.2371 - val_msle: 0.2371 - learning_rate: 1.4788e-04\nEpoch 4/100\n\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 149ms/step - loss: 0.2293 - msle: 0.2293 - val_loss: 0.2132 - val_msle: 0.2132 - learning_rate: 1.4788e-04\nEpoch 5/100\n\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 147ms/step - loss: 0.1986 - msle: 0.1986 - val_loss: 0.1868 - val_msle: 0.1868 - learning_rate: 1.4788e-04\nEpoch 6/100\n\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 147ms/step - loss: 0.1838 - msle: 0.1838 - val_loss: 0.1750 - val_msle: 0.1750 - learning_rate: 1.4788e-04\nEpoch 7/100\n\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 148ms/step - loss: 0.1704 - msle: 0.1704 - val_loss: 0.1659 - val_msle: 0.1659 - learning_rate: 1.4788e-04\nEpoch 8/100\n\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 148ms/step - loss: 0.1631 - msle: 0.1631 - val_loss: 0.1686 - val_msle: 0.1686 - learning_rate: 1.4788e-04\nEpoch 9/100\n\u001b[1m1080/2968\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4:37\u001b[0m 147ms/step - loss: 0.1588 - msle: 0.1588","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"## Plot learning curves\n\nif TRAINING and NN:\n    history_fil = {key: history.history[key] for key in ['msle', 'val_msle']}\n    history_fil2 = {key: history.history[key] for key in ['loss', 'val_loss']}\n    history_fil3 = {key: history.history[key] for key in ['learning_rate']}\n                                                      \n    pd.DataFrame(history_fil).plot()\n    plt.ylabel(\"MSLE\")\n    plt.xlabel(\"epochs\")\n    plt.axis([3, len(history_fil['val_msle']), 0, history_fil['val_msle'][3]+0.1*history_fil['val_msle'][3]])\n    pd.DataFrame(history_fil2).plot()\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"epochs\")\n    plt.axis([3, len(history_fil2['val_loss']), 0, history_fil2['val_loss'][3]+0.1*history_fil2['val_loss'][3]])\n    pd.DataFrame(history_fil3).plot()\n    plt.ylabel(\"Learning rate\")\n    plt.xlabel(\"epochs\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Plot sales for selected store and product family\n\nstore = 44\nfamily = 'AUTOMOTIVE' # SEAFOOD AUTOMOTIVE\nstore_train = f'store_nbr_{store}'\nfamily_train = f'family_{family}'\ndatat_fil = datat[((datat['store_nbr']==store) & (datat['family']==family))]\ntrain_feat_fil = train_feat[((train_feat[store_train]==True) & (train_feat[family_train]==True))]\ntrain_pred_fil = pd.DataFrame(model_nn.predict(train_feat_fil), columns=['prediction'], index=train_feat_fil.index)\ndatat_fil = datat_fil.merge(train_pred_fil, how='left', left_index=True, right_index=True)\ndatat_fil.set_index(['date'], inplace=True)\n\nstart = 31\nplt.figure()\ndatat_fil[['sales', 'prediction']][start:31+start].plot(title='Sales prediction of store {} for {} products'.format(str(store), family))\nplt.figure()\ndatat_fil[['sales', 'prediction']][start:90+start].plot(title='Sales prediction of store {} for {} products'.format(str(store), family))\nplt.figure()\ndatat_fil[['sales', 'prediction']][start:356+start].plot(title='Sales prediction of store {} for {} products'.format(str(store), family))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models=[]\nif ML:\n    models = models + ['svc', 'rfc', 'kneigh', 'gbc', 'xbg']\nif NN:\n    models = models + ['nn']\n\nsubmission_all = pd.DataFrame(data[['id']])\nfor mod in models:\n    model = globals()[f'model_{mod}']\n    submission = pd.DataFrame(data[['id']])\n    test_prediction = pd.DataFrame(model.predict(test_feat), columns=['sales'])\n    submission = submission.merge(test_prediction, how='left', left_index=True, right_index=True)\n    submission.to_csv(f'submission_{mod}.csv', index=False)\n    submission_all[[mod]] = submission[['sales']]\n#submission_all[['Transported']] = pd.DataFrame((submission_all[['svc', 'rfc', 'gbc', 'xbg', 'nn']].sum(axis=1)/5)>0.5)\n#submission_all[['PassengerId', 'Transported']].to_csv('submission_avg.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# # Plot learning curves for definition of start leraning rate\n# lrs = 1e-6 * (10 ** (np.arange(100) / 20)) # Define the learning rate array\n# plt.figure(figsize=(10, 6)) # Set the figure size\n# plt.grid(True) # Set the grid\n# plt.semilogx(lrs, history.history[\"loss\"]) # Plot the loss in log scale\n# plt.tick_params('both', length=10, width=1, which='both') # Increase the tickmarks size\n# plt.axis([1e-5, 1e-1, 0, 1]) # Set the plot boundaries\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}